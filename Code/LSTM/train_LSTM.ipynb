{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4667406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For LSTM model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# For hyperopt (parameter optimization)\n",
    "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
    "from hyperopt.pyll.base import scope  # quniform returns float, some parameters require int; use this to force int\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4028c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e13b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scale_data(data, weeks=1):\n",
    "    X_total = data.values\n",
    "\n",
    "    data = X_total[(53-weeks):, :]\n",
    "    print(data.shape)\n",
    "    scaler, values = utils.scale(data)\n",
    "    values = utils.series_to_supervised(values, n_in=weeks, n_out=1, dropnan=True).values\n",
    "\n",
    "    y_scaler, y = utils.scale(data[:, 1].reshape((len(data), 1)))\n",
    "\n",
    "    train = values[:140, :]\n",
    "    valid1 = values[140:144, :]\n",
    "    valid2 = values[144:148, :]\n",
    "    test = values[148:, :]\n",
    "    \n",
    "\n",
    "    features = 13\n",
    "    obs = weeks*features\n",
    "\n",
    "    y = values[:, -features:]\n",
    "\n",
    "    trainX = train[:, :obs]\n",
    "    trainY = train[:, -features:][:, 1]\n",
    "    validX1 = valid1[:, :obs]\n",
    "    validY1 = valid1[:, -features:][:, 1]\n",
    "    validX2 = valid2[:, :obs]\n",
    "    validY2 = valid2[:, -features:][:, 1]\n",
    "    testX = test[:, :obs]\n",
    "    testY = test[:, -features:][:, 1]\n",
    "\n",
    "    trainX = trainX.reshape((trainX.shape[0], weeks, features))\n",
    "    validX1 = validX1.reshape((validX1.shape[0], weeks, features))\n",
    "    validX2 = validX2.reshape((validX2.shape[0], weeks, features))\n",
    "    testX = testX.reshape((testX.shape[0], weeks, features))\n",
    "    \n",
    "    print(trainX.shape, validX1.shape, validX2.shape, testX.shape)\n",
    "\n",
    "    return trainX, trainY, validX1, validY1, validX2, validY2, testX, testY, scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59153b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    def f_nn(params):\n",
    "        # Generate data with given window\n",
    "        saltlake_week = pd.read_csv('../saltlake_week.csv')\n",
    "        data = saltlake_week[['Cases', 'VMT (Veh-Miles)', 'News Sentiment', 'Unemployment Rate', 'PRCP', 'SNWD',\n",
    "                              'Percent_Fully_Vaccinated_5&Older', 'TAVG',\n",
    "                              'Stay at Home', 'Mask', 'School Opening', 'Health Emergency', 'Holiday']]\n",
    "        trainX, trainY, validX1, validY1, validX2, validY2, testX, testY, scaler, y_scaler = format_scale_data(data=data, \n",
    "                                                                                                               weeks=params['weeks'])\n",
    "        \n",
    "        # Keras LSTM model\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(LSTM(units=params['units_1'], input_shape=(trainX.shape[1], trainX.shape[2]),\n",
    "                       activation='relu', return_sequences=True))                \n",
    "        model.add(Dropout(rate=params['dropout']))\n",
    "        model.add(LSTM(units=params['units_2'],\n",
    "                       activation='relu', return_sequences=True))                \n",
    "        model.add(Dropout(rate=params['dropout']))\n",
    "        model.add(LSTM(units=params['units_3'],\n",
    "                       activation='relu', return_sequences=True))                \n",
    "        model.add(Dropout(rate=params['dropout']))\n",
    "        model.add(LSTM(units=params['units_4'],\n",
    "                       activation='relu', return_sequences=True))                \n",
    "        model.add(Dropout(rate=params['dropout']))\n",
    "        model.add(LSTM(units=params['units_5'],\n",
    "                       activation='relu', return_sequences=True))                \n",
    "        model.add(Dropout(rate=params['dropout']))\n",
    "        model.add(LSTM(units=params['units_6'],\n",
    "                       activation='relu', return_sequences=True))                \n",
    "        model.add(Dropout(rate=params['dropout']))\n",
    "        model.add(LSTM(units=params['units_7'],\n",
    "                       activation='relu', return_sequences=True))                \n",
    "        model.add(Dropout(rate=params['dropout']))\n",
    "        model.add(LSTM(units=params['units_8'],\n",
    "                       activation='relu'))                \n",
    "        model.add(Dropout(rate=params['dropout']))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=params['learning_rate']), \n",
    "                      loss='mean_squared_error', metrics=['mse', 'mape'])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "        result = model.fit(trainX, trainY, verbose=0, validation_data=(validX1, validY1),\n",
    "                            batch_size=4,\n",
    "                            epochs=200,\n",
    "                            callbacks=[es, TqdmCallback(verbose=1)]\n",
    "                            )\n",
    "\n",
    "        # get the lowest validation loss of the training epochs\n",
    "        validation_loss = np.amin(result.history['val_loss'])\n",
    "        print('Best validation loss of epoch:', validation_loss)\n",
    "\n",
    "\n",
    "        return {'loss': validation_loss, 'status': STATUS_OK, 'model': model, 'params': params}\n",
    "\n",
    "    # hyperparameters to search over with hyperopt\n",
    "    space = {'dropout': hp.uniform('dropout', 0.01, 0.5),\n",
    "             'units_1': scope.int(hp.quniform('units_1', 8, 128, 4)),\n",
    "             'units_2': scope.int(hp.quniform('units_2', 8, 128, 4)),\n",
    "             'units_3': scope.int(hp.quniform('units_3', 8, 128, 4)),\n",
    "             'units_4': scope.int(hp.quniform('units_4', 8, 128, 4)),\n",
    "             'units_5': scope.int(hp.quniform('units_5', 8, 128, 4)),\n",
    "             'units_6': scope.int(hp.quniform('units_6', 8, 128, 4)),\n",
    "             'units_7': scope.int(hp.quniform('units_7', 8, 128, 4)),\n",
    "             'units_8': scope.int(hp.quniform('units_8', 8, 128, 4)),\n",
    "             'weeks': scope.int(hp.quniform('weeks', 1, 10, 1)),\n",
    "             'learning_rate': hp.uniform('learning_rate', 0.001, 0.1)\n",
    "             }\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(f_nn, space, algo=tpe.suggest, max_evals=60, trials=trials)\n",
    "\n",
    "    # get best model\n",
    "    best_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
    "    best_params = trials.results[np.argmin([r['loss'] for r in trials.results])]['params']\n",
    "    \n",
    "    # save best model\n",
    "    print(best_params)\n",
    "    print(best_model.summary())\n",
    "    best_model.save('Model/LSTM-8')\n",
    "    \n",
    "    # load data\n",
    "    saltlake_week = pd.read_csv('../saltlake_week.csv')\n",
    "    data = saltlake_week[['Cases', 'VMT (Veh-Miles)', 'News Sentiment', 'Unemployment Rate', 'PRCP', 'SNWD',\n",
    "                          'Percent_Fully_Vaccinated_5&Older', 'TAVG', \n",
    "                          'Stay at Home', 'Mask', 'School Opening', 'Health Emergency', 'Holiday']]\n",
    "    trainX, trainY, validX1, validY1, validX2, validY2, testX, testY, scaler, y_scaler = format_scale_data(data=data, \n",
    "                                                                                                            weeks=best_params['weeks'])\n",
    "    \n",
    "    # evaluate model on second validation set\n",
    "    best_model.evaluate(validX2, validY2)\n",
    "    \n",
    "    # invert predictions of model\n",
    "    yhat_valid2 = best_model.predict(validX2)\n",
    "    yhat_valid2_inv = y_scaler.inverse_transform(yhat_valid2)\n",
    "    validY2_inv = y_scaler.inverse_transform(validY2.reshape((-1, 1)))\n",
    "    \n",
    "    print(yhat_valid2.shape)\n",
    "    print(yhat_valid2_inv.shape)\n",
    "    print(validY2_inv.shape)\n",
    "    \n",
    "    yhat_train = best_model.predict(trainX)\n",
    "    yhat_train_inv = y_scaler.inverse_transform(yhat_train)\n",
    "    trainY_inv = y_scaler.inverse_transform(trainY.reshape((-1, 1)))\n",
    "    \n",
    "    # evaluate model with inverted features\n",
    "    print(\"Mean Squared Error: {}\".format(mean_squared_error(validY2_inv, yhat_valid2_inv)))\n",
    "    print(\"Root Mean Squared Error: {}\".format(mean_squared_error(validY2_inv, yhat_valid2_inv, squared=False)))\n",
    "    print(\"Mean Absolute Percentage Error: {}\".format(mean_absolute_percentage_error(validY2_inv, yhat_valid2_inv)))\n",
    "    \n",
    "    # plot model predictions\n",
    "    plt.figure()\n",
    "    plt.plot(yhat_valid2_inv, label='Predicted')\n",
    "    plt.plot(validY2_inv, label='True')\n",
    "    plt.legend()\n",
    "    plt.title(\"Validation Data 2\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(yhat_train_inv, label='Predicted')\n",
    "    plt.plot(trainY_inv, label='True')\n",
    "    plt.legend()\n",
    "    plt.title(\"Training Data\")\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2ec67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model8 = run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
